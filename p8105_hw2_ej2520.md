Homework 2: Data Wrangling
================

## Problem 1

This is the start of Homework 2 for P8105: Data Science 1. First, we are
going to attach the packages needed for this homework.

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.1     ✔ tibble    3.2.1
    ## ✔ lubridate 1.9.3     ✔ tidyr     1.3.1
    ## ✔ purrr     1.0.2     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

``` r
library(readxl)
```

Next, we are going to import and clean the NYC Transit Subway dataset.

``` r
transit_df = 
  read_csv(file = "NYC_Transit_Subway_Entrance_And_Exit_Data.csv",
  col_types = cols(Route8 = "c", Route9 = "c", Route10 = "c", Route11 = "c")) |>
janitor::clean_names() |>
  select(
    line, station_name, station_latitude, station_longitude, starts_with("route"),
    entry, exit_only, vending, entrance_type, ada) |>
  mutate(entry = ifelse(entry == "YES", TRUE, FALSE))
```

How many distinct stations are there?

``` r
transit_df |> 
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 465 × 2
    ##    station_name             line    
    ##    <chr>                    <chr>   
    ##  1 25th St                  4 Avenue
    ##  2 36th St                  4 Avenue
    ##  3 45th St                  4 Avenue
    ##  4 53rd St                  4 Avenue
    ##  5 59th St                  4 Avenue
    ##  6 77th St                  4 Avenue
    ##  7 86th St                  4 Avenue
    ##  8 95th St                  4 Avenue
    ##  9 9th St                   4 Avenue
    ## 10 Atlantic Av-Barclays Ctr 4 Avenue
    ## # ℹ 455 more rows

How many stations are ADA complaint? 84 stations.

``` r
transit_df |> 
  filter(ada == "TRUE") |>
  select(station_name, line) |> 
  distinct()
```

    ## # A tibble: 84 × 2
    ##    station_name                   line           
    ##    <chr>                          <chr>          
    ##  1 Atlantic Av-Barclays Ctr       4 Avenue       
    ##  2 DeKalb Av                      4 Avenue       
    ##  3 Pacific St                     4 Avenue       
    ##  4 Grand Central                  42nd St Shuttle
    ##  5 34th St                        6 Avenue       
    ##  6 47-50th Sts Rockefeller Center 6 Avenue       
    ##  7 Church Av                      6 Avenue       
    ##  8 21st St                        63rd Street    
    ##  9 Lexington Av                   63rd Street    
    ## 10 Roosevelt Island               63rd Street    
    ## # ℹ 74 more rows

What proportion of station entrances/exits without vending allow
entrance?

``` r
transit_df |> 
  filter(vending == "NO") |>
  pull(entry) |>
  mean()
```

    ## [1] 0.3770492

How many distinct stations serve the A train? Of the stations that serve
the A train, how many are ADA complaint?

60 stations serve the A train. Of the stations that serve the A train,
none are ADA complaint.

``` r
transit_df |> 
  pivot_longer(
    route1:route11, 
    names_to = "route_num", 
    values_to = "route") |>
  filter(route == "A") |>
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 60 × 2
    ##    station_name                  line           
    ##    <chr>                         <chr>          
    ##  1 Times Square                  42nd St Shuttle
    ##  2 125th St                      8 Avenue       
    ##  3 145th St                      8 Avenue       
    ##  4 14th St                       8 Avenue       
    ##  5 168th St - Washington Heights 8 Avenue       
    ##  6 175th St                      8 Avenue       
    ##  7 181st St                      8 Avenue       
    ##  8 190th St                      8 Avenue       
    ##  9 34th St                       8 Avenue       
    ## 10 42nd St                       8 Avenue       
    ## # ℹ 50 more rows

``` r
transit_df |> 
  pivot_longer(
    route1:route11,
    names_to = "route") |>
  filter(route == "A", ada == TRUE) |>
  select(station_name, line) |>
  distinct()
```

    ## # A tibble: 0 × 2
    ## # ℹ 2 variables: station_name <chr>, line <chr>

## Problem 2

This is the start of Problem 2. First, we are going to import the
Mr. Trash Wheel dataset.We will clean the variable names, and convert
the sports_balls variable into an integer variable.

``` r
library(readxl)

trash_df = 
  read_excel("202409_Trash_Wheel_Collection_Data.xlsx",
    na = c("NA", "")) |> 
  janitor::clean_names() |> 
  mutate(sports_balls = as.integer(round(sports_balls))) |>
  mutate(year = as.numeric(year)) |>
  mutate(mr_trash_wheel = plastic_bottles + plastic_bags) 
```

    ## New names:
    ## • `` -> `...15`
    ## • `` -> `...16`

Next, we are importing the Professor Trash Wheel sheet from the Trash
Wheel dataset.

``` r
professor_df = 
  read_excel("202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Professor Trash Wheel",
    na = c("NA", "")) |>
  janitor::clean_names() |>
  mutate(professor_wheel = plastic_bottles + plastic_bags)
```

Now, we are importing the Gwynnda Trash Wheel sheet from the Trash Wheel
dataset.

``` r
Gwynnda_df = 
  read_excel("202409_Trash_Wheel_Collection_Data.xlsx", sheet = "Gwynnda Trash Wheel",
    na = c("NA", "")) |>
  janitor::clean_names() |> 
mutate(Gwynnda_wheel = plastic_bottles + plastic_bags)
```

As the next step to this problem, we are going to combine the Mr. Trash
Wheel, Professor Trash Wheel, and the Gwynnda Trash Wheel datasets.

``` r
trash_tidy = 
  bind_rows(trash_df, professor_df, Gwynnda_df) |> 
  janitor:: clean_names() |> 
  pivot_longer(
    mr_trash_wheel:gwynnda_wheel,
    names_to = "Dataset_type", 
    values_to = "plastic")
```

Our combined data has 3,114 observations and 18 variables.Some key
variables are weight_tons, volume_cubic_yards, plastic_bottles,
polystyrene, cigarette_butts, and glass_bottles.

``` r
trash_tidy |>
  filter(Dataset_type == "professor_wheel") |>
  pull(weight_tons)
```

    ##    [1]    4.31    2.74    3.45    3.10    4.06    2.71    1.91    3.70    2.52
    ##   [10]    3.76    3.43    4.17    5.13    4.17    3.28    3.05    2.49    2.54
    ##   [19]    2.41    3.83    2.73    4.40    2.79    2.50    4.39    5.33    3.58
    ##   [28]    3.10    1.77    3.76    1.24    3.14    2.71    2.11    4.09    2.31
    ##   [37]    3.50    2.70    3.25    3.84    2.88    1.81    3.48    3.18    2.87
    ##   [46]    2.00    2.14    2.54    3.82    1.83    2.02    2.50    3.41    1.83
    ##   [55]    3.84    3.22    3.03    2.64    3.54    1.95    4.25    5.62    3.81
    ##   [64]    3.45    3.34    2.54    4.66    3.39    3.17    3.15    3.83    4.78
    ##   [73]    3.85    4.48    4.18    3.38    3.83    3.95    2.44    2.85    3.43
    ##   [82]    4.28    3.94    3.38    3.45    3.93    4.07    3.03    3.52    3.51
    ##   [91]    3.68    3.40    3.72    1.84    3.00    4.33    4.30    3.39    4.08
    ##  [100]    3.62    2.40    4.24    3.31    2.90    2.46    2.89    4.46    3.42
    ##  [109]    3.56    1.92    2.90    3.24    4.41    3.39    3.27    3.60    4.06
    ##  [118]    2.11    3.01    3.41    3.66    4.51    2.89    2.89    2.51    3.58
    ##  [127]    3.41    2.79    4.21    4.88    4.47    3.29    4.16    2.93    2.37
    ##  [136]    2.57    2.39    3.98    3.47    3.97    3.12    3.17    2.12    2.31
    ##  [145]    3.16    4.02    3.26    3.82    4.88    5.05    2.79    3.47    4.01
    ##  [154]    3.18    3.13    2.43    3.08    2.90    3.00    2.01    1.98    2.75
    ##  [163]    3.41    2.55    1.74    2.13    2.43    2.32    3.72    2.14    2.74
    ##  [172]    2.63    2.09    2.67    2.67    3.55    3.89    3.59    3.13    3.33
    ##  [181]    3.11    3.04    3.21    4.08    3.26    3.36    3.62    4.27    4.01
    ##  [190]    3.88    4.67    3.53    4.07    3.93    2.59    3.81    3.30    3.84
    ##  [199]    4.10    4.10    2.52    1.95    3.58    1.50    3.63    2.21    4.04
    ##  [208]    3.87    3.68    2.17    2.10    3.70    3.39    3.14    3.54    3.07
    ##  [217]    2.15    2.76    2.76    2.38    2.02    2.17    2.08    3.12    2.45
    ##  [226]    3.72    4.00    3.54    3.19    4.51    2.97    3.78    2.66    2.64
    ##  [235]    4.30    2.91    3.62    2.40    3.19    3.03    3.26    3.04    3.09
    ##  [244]    2.72    3.65    3.38    3.44    2.57    3.36    2.72    3.81    3.36
    ##  [253]    4.29    4.29    4.00    3.25    1.60    3.62    3.19    2.86    4.10
    ##  [262]    3.38    3.35    3.17    4.19    2.60    4.15    4.23    4.15    4.44
    ##  [271]    3.12    0.96    2.99    3.49    4.39    3.57    3.79    4.24    3.49
    ##  [280]    4.16    3.17    3.31    3.32    3.98    3.84    3.31    4.62    3.74
    ##  [289]    3.27    3.04    2.43    3.35    3.50    4.25    3.77    2.26    3.09
    ##  [298]    2.95    3.28    3.10    3.04    3.11    2.26    3.46    4.54    3.19
    ##  [307]    3.17    2.78    2.77    3.26    3.53    3.38    2.96    3.62    2.50
    ##  [316]    3.07    2.33    3.24    2.52    2.27    3.06    3.33    3.03    2.60
    ##  [325]    2.26    3.52    3.84    3.57    5.10    3.53    3.04    3.07    3.23
    ##  [334]    3.73    3.38    2.92    2.83    2.76    2.51    2.72    3.19    3.23
    ##  [343]    3.08    3.02    2.08    2.03    2.48    3.34    2.69    2.57    3.45
    ##  [352]    2.25    2.48    3.13    2.65    3.96    2.61    3.82    2.66    4.25
    ##  [361]    2.96    4.00    3.17    3.07    2.41    3.21    3.25    3.43    3.28
    ##  [370]    2.66    2.98    3.05    2.23    3.20    3.20    3.58    3.88    2.25
    ##  [379]    2.72    2.78    2.75    1.80    2.90    2.54    2.18    3.24    3.07
    ##  [388]    2.80    2.80    1.90    2.34    2.15    3.86    3.98    4.62    3.36
    ##  [397]    2.91    3.50    3.13    3.06    3.17    4.22    3.68    4.00    3.98
    ##  [406]    2.55    0.78    3.04    3.00    3.79    3.84    3.96    3.29    3.35
    ##  [415]    3.44    3.78    2.93    3.34    2.14    3.10    3.05    3.11    3.32
    ##  [424]    2.92    2.98    1.89    2.70    1.41    3.52    3.39    2.36    3.19
    ##  [433]    3.02    2.98    3.39    4.29    3.62    2.45    2.92    3.07    2.42
    ##  [442]    3.11    2.92    1.94    3.02    2.79    2.01    3.35    3.34    3.08
    ##  [451]    2.73    2.12    2.81    2.43    2.69    1.83    3.14    2.72    3.34
    ##  [460]    3.64    2.95    2.34    2.55    3.50    5.03    3.90    3.66    3.69
    ##  [469]    3.77    4.27    3.03    3.83    3.30    3.90    2.78    2.67    3.58
    ##  [478]    4.50    3.91    1.32    2.31    3.66    2.07    3.80    3.97    5.20
    ##  [487]    2.00    1.66    2.18    4.01    2.90    4.12    4.00    2.92    3.38
    ##  [496]    3.81    2.61    3.02    2.54    3.09    3.73    3.89    3.90    3.72
    ##  [505]    2.66    3.55    2.80    3.22    1.90    1.83    2.20    2.99    3.12
    ##  [514]    2.86    2.72    2.15    2.71    4.66    2.35    2.75    2.59    3.70
    ##  [523]    2.61    3.97    4.51    2.83    4.14    3.21    3.83    3.97    4.02
    ##  [532]    3.76    3.18    4.02    3.36    1.64    3.76    1.97    3.96    3.77
    ##  [541]    3.25    2.75    2.70    3.39    2.82    2.03    1.69    2.84    2.25
    ##  [550]    3.09    3.67    3.64    3.78    3.69    3.88    4.18    2.73    4.39
    ##  [559]    2.03    3.12    2.27    2.54    3.27    2.79    3.15    3.02    3.36
    ##  [568]    3.23    3.03    3.53    3.14    3.41    4.78    5.31    2.97    3.66
    ##  [577]    4.87    5.31    3.66    2.62    3.56    3.79    2.28    3.90    4.08
    ##  [586]    4.36    4.23    3.71    3.58    3.79    3.88    3.81    3.82    1.89
    ##  [595]    3.58    3.67    2.91    4.44    3.41    3.83    2.67    2.15    3.08
    ##  [604]    2.77    3.55    3.51    3.32    3.10    3.97    2.86    3.53    2.39
    ##  [613]    3.26    3.55    2.27    2.19    3.36    2.93    2.46    3.23    2.07
    ##  [622]    2.63    2.95    3.18    3.73    3.01    2.34    2.88    2.51    3.18
    ##  [631]    2.20    3.00    2.46    2.82    2.92    2.61    3.15    2.96    3.65
    ##  [640]    2.54    3.34    4.30    3.49    1.25    4.90    3.68    4.70    4.13
    ##  [649]    3.34    3.02    4.00      NA 2054.37    1.79    1.58    2.32    3.72
    ##  [658]    1.45    1.71    1.82    2.37    2.64    2.78    2.34    1.63    2.37
    ##  [667]    1.69    2.93    1.21    1.47    0.75    0.99    1.58    1.23    2.32
    ##  [676]    1.73    1.78    1.98    1.37    1.85    2.84    1.30    1.81    1.66
    ##  [685]    2.12    1.23    1.80    2.46    1.58    0.97    1.57    2.41    3.18
    ##  [694]    2.28    1.77    1.88    0.73    1.37    2.59    1.89    2.58    2.01
    ##  [703]    1.91    1.10    3.33    2.54    2.52    2.02    1.81    1.76    0.94
    ##  [712]    1.71    1.81    2.72    1.61    2.43    2.29    1.97    1.52    0.61
    ##  [721]    1.18    1.82    2.79    1.68    1.21    2.48    2.49    2.57    1.96
    ##  [730]    2.91    2.33    2.58    3.34    3.18    1.99    3.18    1.99    1.50
    ##  [739]    1.32    0.93    2.01    2.61    3.46    2.76    2.41    2.43    2.98
    ##  [748]    2.50    1.97    3.49    1.32    1.61    1.98    2.13    2.41    2.05
    ##  [757]    2.58    1.85    2.25    3.42    2.62    2.64    2.82    1.71    2.43
    ##  [766]    1.28    2.13    2.20    3.75    3.00    2.48      NA      NA  241.26
    ##  [775]    0.93    2.26    1.62    1.76    1.53    2.06    1.90    2.16    2.60
    ##  [784]    3.21    2.44    2.62    2.92    2.93    3.31    2.70    2.10    2.60
    ##  [793]    3.28    2.89    2.49    2.36    1.84    3.09    3.54    2.25    3.38
    ##  [802]    3.20    3.71    4.06    3.21    3.26    3.95    2.79    3.02    2.97
    ##  [811]    3.00    3.23    3.36    3.57    3.27    3.43    3.91    3.76    4.13
    ##  [820]    4.08    4.02    3.63    2.59    1.51    1.68    2.78    2.53    2.53
    ##  [829]    3.08    2.75    0.77    2.76    2.46    2.80    2.82    1.99    2.78
    ##  [838]    2.83    2.26    2.13    2.19    2.12    3.93    3.59    3.27    3.22
    ##  [847]    3.39    2.67    3.68    2.95    3.00    2.34    3.23    3.07    3.74
    ##  [856]    3.39    3.49    3.40    3.69    3.55    1.00    3.23    3.08    3.07
    ##  [865]    3.58    3.50    3.09    3.56    3.16    4.18    3.11    2.83    2.82
    ##  [874]    3.32    3.40    3.68    3.50    3.51    3.29    2.58    2.62    3.37
    ##  [883]    1.93    2.66    3.01    3.78    3.40    3.19    3.35    2.86    2.87
    ##  [892]    3.06    3.00    2.42    2.37    2.91    2.35    2.80    2.69    2.27
    ##  [901]    2.50    2.72    2.74    3.08    3.16    3.04    3.09    3.15    2.48
    ##  [910]    3.20    3.08    2.47    2.69    2.41    2.91    2.35    3.25    3.40
    ##  [919]    2.81    2.85    3.22    3.44    1.93    2.68    2.74    3.12    3.12
    ##  [928]    3.45    2.88    3.62    3.22    2.96    3.09    3.52    3.61    3.16
    ##  [937]    3.24    2.90    3.19    2.70    2.99    2.32    2.79    2.60    2.50
    ##  [946]    2.28    2.78    2.79    2.22    2.93    2.48    2.17    0.93    3.10
    ##  [955]    2.44    3.80    3.67    2.84    3.59    3.15    3.52    2.98    3.50
    ##  [964]    3.34    1.87    2.51    2.64    2.82    2.75    2.90    2.98    2.99
    ##  [973]    3.44    3.27    2.99    2.99    2.99    3.85    3.36    3.18    2.86
    ##  [982]    3.52    3.88    3.95    3.48    4.24    3.58    3.96    3.45    3.96
    ##  [991]    3.38    4.02    3.49    3.29    3.15    3.61    3.34    3.56    3.99
    ## [1000]    3.31    3.50    3.97    3.81    3.95    3.84    3.28    4.09    3.56
    ## [1009]    3.47    3.72    3.95    3.84    3.91    3.52    4.02    3.27    2.89
    ## [1018]    3.01    3.44    2.68    2.70    2.30    2.29    3.07    3.06    3.08
    ## [1027]    2.93    3.43    2.87    3.27    2.72    3.00    3.78    3.35    3.55
    ## [1036]    2.88    3.43  759.26

``` r
sum()
```

    ## [1] 0

## Problem 3

First, I am going to import and clean the individual bakers dataset.

``` r
bakers_df = 
  read_csv("gbb_datasets/bakers.csv",
    na = c("NA", "", ".")) |>
  janitor::clean_names() |>
  select(series, everything()) |>
  arrange(series)
```

    ## Rows: 120 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker Name, Baker Occupation, Hometown
    ## dbl (2): Series, Baker Age
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Next, we are importing and cleaning the bakes dataset.

``` r
bakes_df = 
  read_csv("gbb_datasets/bakes.csv",
    na = c("NA", "", ".")) |>
  janitor::clean_names()
```

    ## Rows: 548 Columns: 5
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (3): Baker, Signature Bake, Show Stopper
    ## dbl (2): Series, Episode
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

Finally, we are importing and cleaning the results dataset. For the
results dataset, I renamed the columns “series”, “episode”, “baker”,
“technical”, and “results.” Next, I am going to remove the first three
rows of the results dataset, and then change the series and episode
variables to numeric. Should I have changed the results variable to
something other than character?

``` r
results_df = 
  read_csv("gbb_datasets/results.csv", 
    na = c("NA", "", "."), col_names = c("series", "episode", "baker", "technical","result"), col_types = cols('series' = col_double(), 'episode' = col_double(), 'technical' = col_double())) |>
  janitor::clean_names() 
```

    ## Warning: One or more parsing issues, call `problems()` on your data frame for details,
    ## e.g.:
    ##   dat <- vroom(...)
    ##   problems(dat)

``` r
results_df <- results_df[-c(1,2,3),]
```

Merging the datasets.

``` r
bakers_mid <- bakers_df |>
  anti_join(bakes_df, by = "series")

bake_final <- bakers_mid |>
  anti_join(results_df, by = "series")
```

Exporting the datasets.

``` r
write_csv(bake_final, "bake_final.csv")
```
